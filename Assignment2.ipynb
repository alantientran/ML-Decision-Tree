{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This assignment may be worked individually or in pairs. Enter your name/s here:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# names here\n",
    "# Alan Tran \n",
    "# John R. Smith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 2: Decision Trees\n",
    "\n",
    "In this assignment we'll implement the Decision Tree algorithm to classify patients as either having or not having diabetic retinopathy. For this task we'll be using the Diabetic Retinopathy data set, which contains features from the Messidor image set to predict whether an image contains signs of diabetic retinopathy or not. This dataset has `1150` records and `20` attributes (some categorical, some continuous). You can find additional details about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Information:\n",
    "\n",
    "0) The binary result of quality assessment. 0 = bad quality 1 = sufficient quality.\n",
    "\n",
    "1) The binary result of pre-screening, where 1 indicates severe retinal abnormality and 0 its lack. \n",
    "\n",
    "2-7) The results of MA detection. Each feature value stand for the number of MAs found at the confidence levels alpha = 0.5, . . . , 1, respectively. \n",
    "\n",
    "8-15) contain the same information as 2-7) for exudates. However, as exudates are represented by a set of points rather than the number of pixels constructing the lesions, these features are normalized by dividing the \n",
    "number of lesions with the diameter of the ROI to compensate different image sizes. \n",
    "\n",
    "16) The euclidean distance of the center of the macula and the center of the optic disc to provide important information regarding the patient's condition. This feature is also normalized with the diameter of the ROI.\n",
    "\n",
    "17) The diameter of the optic disc. \n",
    "\n",
    "18) The binary result of the AM/FM-based classification.\n",
    "\n",
    "19) Class label. 1 = contains signs of Diabetic Retinopathy, 0 = no signs of Diabetic Retinopathy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation: \n",
    "The function prototypes are given to you, please don't change those. You can add additional helper functions if needed. \n",
    "\n",
    "*Suggestion:* The dataset is substantially big, for the purpose of easy debugging, work with a subset of the data and test your decision tree implementation on that.\n",
    "\n",
    "#### Notes:\n",
    "Parts of this assignment will be **autograded** so a couple of caveats :-\n",
    "- Entropy is calculated using log with base 2, `math.log2(x)`.\n",
    "- For continuous features ensure that the threshold value lies exactly between 2 values. For example, if for feature 2 the best split occurs between 10 and 15 then the threshold value will be set as 12.5. For binary features [0/1] the threshold value will be 0.5.\n",
    "- All values < `thresh_val` go to the left child and all values >= `thresh_val` go to the right child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers if you wish\n",
    "# EXCEPT for scikit-learn... You may NOT use scikit-learn for this assignment!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataPoint:\n",
    "    def __str__(self):\n",
    "        return \"< \" + str(self.label) + \": \" + str(self.features) + \" >\"\n",
    "    \n",
    "    def __init__(self, label, features):\n",
    "        self.label = label # the classification label of this data point\n",
    "        self.features = features # a list of feature values for this data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Read data from a CSV file. Put it into a list of `DataPoints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = []\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "    \n",
    "    for index, row in df.iterrows() : \n",
    "        dp = DataPoint(label=row[19], features=row[0:19]) # 19 is the class label, whether someone has stuff\n",
    "        data.append(dp)\n",
    "    return data\n",
    "silly_data = get_data('subset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    is_leaf = True          # boolean variable to check if the node is a leaf\n",
    "    feature_idx = None      # index that identifies the feature\n",
    "    thresh_val = None       # threshold value that splits the node\n",
    "    prediction = None       # prediction class (only valid for leaf nodes)\n",
    "    left_child = None       # left TreeNode (all values < thresh_val)\n",
    "    right_child = None      # right TreeNode (all values >= thresh_val)\n",
    "    \n",
    "    def printTree(self, level=0):    # for debugging purposes\n",
    "        if self.is_leaf:\n",
    "            print ('-'*level + 'Leaf Node:      predicts ' + str(self.prediction))\n",
    "        else:\n",
    "            print ('-'*level + 'Internal Node:  splits on feature ' \n",
    "                   + str(self.feature_idx) + ' with threshold ' + str(self.thresh_val))\n",
    "            self.left_child.printTree(level+1)\n",
    "            self.right_child.printTree(level+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Implement the function `make_prediction` that takes the decision tree root and a `DataPoint` instance and returns the prediction label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(tree_root, data_point):\n",
    "#     your code goes here\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Implement the function `split_dataset` given an input data set, a `feature_idx` and the `threshold` for the feature. `left_split` will have all values < `threshold` and `right_split` will have all values >= `threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(data, feature_idx, threshold):\n",
    "    left_split = []\n",
    "    right_split = []\n",
    "    for dp in data:\n",
    "        if dp.features[feature_idx] < threshold :\n",
    "            left_split.append(dp)\n",
    "        else :\n",
    "            right_split.append(dp)\n",
    "#     your code goes here\n",
    "    return (left_split, right_split)\n",
    "\n",
    "test_l, test_r = split_dataset(silly_data, 1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Implement the function `calc_entropy` to return the entropy of the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_entropy(data):\n",
    "    entropy = 0.0\n",
    "    yes = 0\n",
    "    no = 0\n",
    "    for dp in data :\n",
    "        if (dp.label == 1) :\n",
    "            yes = yes + 1\n",
    "        else : \n",
    "            no = no + 1\n",
    "    sum = yes + no\n",
    "\n",
    "    # print(\"num yes: \", yes)\n",
    "    # print(\"num no: \", no)\n",
    "    # print(\"num sum: \", sum)\n",
    "    if yes == 0 or no == 0 :\n",
    "        return 0\n",
    "    else : \n",
    "        return -(yes/sum) * log2(yes/sum) - (no/sum) * log2(no/sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Q5. Implement the function `calc_best_threshold` which returns the best information gain and the corresponding threshold value for one feature at `feature_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_best_threshold(data, feature_idx):\n",
    "    best_info_gain = 0.0\n",
    "    best_thresh = None\n",
    "    parent_entropy = calc_entropy(data)\n",
    "    ft_values = []\n",
    "\n",
    "    # sort data by feature_idx\n",
    "    data = sorted(data, key=lambda x: x.features[feature_idx])\n",
    "    index = 1\n",
    "\n",
    "    for index in range(len(data)) :\n",
    "        # Check if right and left node feature values differ\n",
    "        if (index > 0 and data[index-1].label != data[index].label) : \n",
    "            if (data[index-1].features[feature_idx] == data[index].features[feature_idx]):\n",
    "                # check to the left\n",
    "                left_bound = index\n",
    "                while (left_bound-2 > 0 and data[left_bound-2].features[feature_idx] \n",
    "                            == data[left_bound-1].features[feature_idx]) :\n",
    "                    \n",
    "                    left_bound = left_bound-1\n",
    "                left_thresh = (data[left_bound-2].features[feature_idx] + data[left_bound-1].features[feature_idx])/2\n",
    "                info_gain_left = calc_gain_at_thresh(left_thresh, data, feature_idx, parent_entropy)\n",
    "                # print(\"left case, gain: \", info_gain_left)\n",
    "                # print(\"left case, threshold: \", left_thresh)\n",
    "                if info_gain_left > best_info_gain: \n",
    "                    best_info_gain = info_gain_left\n",
    "                    best_thresh = left_thresh\n",
    "\n",
    "                # check to the right\n",
    "                right_bound = index\n",
    "                while (right_bound + 1 < len(data) and data[right_bound].features[feature_idx] \n",
    "                            == data[right_bound+1].features[feature_idx]) :\n",
    "                    \n",
    "                    right_bound = right_bound+1\n",
    "                right_thresh = (data[left_bound-2].features[feature_idx] + data[right_bound-1].features[feature_idx])/2\n",
    "                info_gain_right = calc_gain_at_thresh(right_thresh, data, feature_idx, parent_entropy)\n",
    "                # print(\"right case, gain: \", info_gain_right)\n",
    "                # print(\"right case, threshold: \", right_thresh)\n",
    "                if info_gain_right > best_info_gain: \n",
    "                    best_info_gain = info_gain_right\n",
    "                    best_thresh = right_thresh\n",
    "                \n",
    "            else : \n",
    "                cur_thresh = (data[index-1].features[feature_idx] + data[index].features[feature_idx])/2\n",
    "                cur_info_gain = calc_gain_at_thresh(cur_thresh, data, feature_idx, parent_entropy)\n",
    "                # print(\"default case, gain: \", cur_info_gain)\n",
    "                # print(\"default case, threshold: \", cur_thresh)\n",
    "\n",
    "                if cur_info_gain > best_info_gain : \n",
    "                    best_info_gain = cur_info_gain\n",
    "                    best_thresh = cur_thresh\n",
    "    return (best_info_gain, best_thresh)\n",
    "\n",
    "\n",
    "\n",
    "def calc_gain_at_thresh(threshold, data, feature_idx, parent_entropy) :\n",
    "    left, right = split_dataset(data, feature_idx, threshold)\n",
    "    e_left = calc_entropy(left)\n",
    "    e_right = calc_entropy(right)\n",
    "    cur_entropy = ((len(left)/len(data)) * e_left) + ((len(right)/len(data)) * e_right)\n",
    "    cur_info_gain = parent_entropy - cur_entropy\n",
    "    # print(cur_info_gain)\n",
    "    # print(cur_thresh)\n",
    "\n",
    "    return cur_info_gain\n",
    "\n",
    "# for i in range(2, 18) :\n",
    "test_info_gain, test_threshold = calc_best_threshold(silly_data, 2)\n",
    "# print(\"best info gain: \", test_info_gain)\n",
    "# print(\"best threshold:  \", test_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Implement the function `identify_best_split` which returns the best feature to split on for an input dataset, and also returns the corresponding threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_best_split(data):\n",
    "    # print (\"data length in split: \", len(data))\n",
    "    # print(\"entropy of the data: \", calc_entropy(data))\n",
    "    if len(data) < 2:\n",
    "        return (None, None)\n",
    "    best_feature = None\n",
    "    best_thresh = None\n",
    "    best_info_gain = 0.0\n",
    "    cur_threshold = None \n",
    "    cur_info_gain = None\n",
    "    parent_entropy = calc_entropy(data)\n",
    "    for ft in range(0, 19) :\n",
    "        # print(\"--Check feature \", ft)\n",
    "        # for dp_idx in range(len(data)) : \n",
    "        if data[0].features[ft] > 1 : \n",
    "            # This is a continuous feature, so we need to worry about thresholds \n",
    "            cur_info_gain, cur_threshold = calc_best_threshold(data, ft)\n",
    "            # print(\"cur threshold: \", cur_threshold)\n",
    "            # print(\"cur info gain for that threshold: \", cur_info_gain)\n",
    "        else : \n",
    "            # This is a categorical feature. The threshold will always be 0.5 for these features\n",
    "            cur_threshold = 0.5\n",
    "            left, right = split_dataset(data, ft, cur_threshold)\n",
    "            entropy_left = calc_entropy(left)\n",
    "            entropy_right = calc_entropy(right)\n",
    "            # print(\"length of left split: \", len(left))\n",
    "            # print(\"length of right split: \", len(right))\n",
    "\n",
    "            total_entropy = (len(left) / len (data) * entropy_left) + ((len(right) / len(data)) * entropy_right)\n",
    "            # print(\"total entropy: \", total_entropy)\n",
    "            cur_info_gain = parent_entropy - total_entropy\n",
    "            # print(\"current info gain: \", cur_info_gain)\n",
    "        if (cur_info_gain > best_info_gain) :\n",
    "            best_info_gain = cur_info_gain\n",
    "            best_feature = ft\n",
    "            best_thresh = cur_threshold\n",
    "    # print(\"best info gain: \", best_info_gain)\n",
    "    return (best_feature, best_thresh)\n",
    "\n",
    "my_feature, my_thresh = identify_best_split(silly_data)\n",
    "# print(\"best feature: \", my_feature, \"best thresh: \", my_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Implement the function `create_leaf_node` which returns a `TreeNode` with `is_leaf=True` and `prediction` set to whichever classification occurs most in the dataset at this node. If there is a tie, choose classification label 1 (has disease). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf_node(data):\n",
    "    node = TreeNode\n",
    "    node.is_leaf=True\n",
    "\n",
    "    yes = 0\n",
    "    no = 0\n",
    "    for dp in data : \n",
    "        if dp.label : \n",
    "            yes = yes + 1\n",
    "        else : \n",
    "            no = no + 1\n",
    "    if (yes >= no) :\n",
    "        node.prediction = 1\n",
    "    else :\n",
    "        node.prediction = 0\n",
    "\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Implement the `create_decision_tree` function. `max_levels` denotes the maximum height of the tree (for example if `max_levels = 1` then the decision tree will only contain the leaf node at the root). [Hint: this is where the recursion happens.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent left length:  8\n",
      "parent right length:  2\n",
      "ROOT split on  18\n",
      "Child of parent feature   18  split on new feature  8\n",
      "child height  0  length left child:  4\n",
      "child height  0  length right child:  4\n",
      "8  reached base case\n",
      "Child of parent feature   8  split on new feature  2\n",
      "child height  1  length left child:  1\n",
      "child height  1  length right child:  3\n",
      "2  reached base case\n",
      "Child of parent feature   2  split on new feature  2\n",
      "child height  2  length left child:  2\n",
      "child height  2  length right child:  1\n",
      "2  reached base case\n",
      "2  reached base case\n",
      "created new left child with index:  2\n",
      "created new right child with index:  2\n",
      "created new left child with index:  2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'feature_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 46\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated new left child with index: \u001b[39m\u001b[38;5;124m\"\u001b[39m, child\u001b[38;5;241m.\u001b[39mleft_child\u001b[38;5;241m.\u001b[39mfeature_idx)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated new right child with index: \u001b[39m\u001b[38;5;124m\"\u001b[39m, child\u001b[38;5;241m.\u001b[39mright_child\u001b[38;5;241m.\u001b[39mfeature_idx)\n\u001b[0;32m---> 46\u001b[0m test_tree \u001b[38;5;241m=\u001b[39m create_decision_tree(silly_data, \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# test_tree.is_leaf = False\u001b[39;00m\n\u001b[1;32m     48\u001b[0m TreeNode\u001b[38;5;241m.\u001b[39mprintTree(test_tree)\n",
      "Cell \u001b[0;32mIn[158], line 13\u001b[0m, in \u001b[0;36mcreate_decision_tree\u001b[0;34m(data, max_levels)\u001b[0m\n\u001b[1;32m      9\u001b[0m root_node\u001b[38;5;241m.\u001b[39mis_leaf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROOT split on \u001b[39m\u001b[38;5;124m\"\u001b[39m, my_feature)\n\u001b[0;32m---> 13\u001b[0m root_node\u001b[38;5;241m.\u001b[39mleft_child \u001b[38;5;241m=\u001b[39m tree_helper(left, \u001b[38;5;241m0\u001b[39m, max_levels, root_node)\n\u001b[1;32m     14\u001b[0m root_node\u001b[38;5;241m.\u001b[39mright_child \u001b[38;5;241m=\u001b[39m tree_helper(right, \u001b[38;5;241m0\u001b[39m, max_levels, root_node)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m root_node\n",
      "Cell \u001b[0;32mIn[158], line 40\u001b[0m, in \u001b[0;36mtree_helper\u001b[0;34m(data_sub, cur_height, max_levels, cur_node)\u001b[0m\n\u001b[1;32m     37\u001b[0m new_height \u001b[38;5;241m=\u001b[39m cur_height \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     39\u001b[0m child\u001b[38;5;241m.\u001b[39mleft_child \u001b[38;5;241m=\u001b[39m tree_helper(left, new_height, max_levels, child)\n\u001b[0;32m---> 40\u001b[0m child\u001b[38;5;241m.\u001b[39mright_child \u001b[38;5;241m=\u001b[39m tree_helper(right, new_height, max_levels, child)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated new left child with index: \u001b[39m\u001b[38;5;124m\"\u001b[39m, child\u001b[38;5;241m.\u001b[39mleft_child\u001b[38;5;241m.\u001b[39mfeature_idx)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated new right child with index: \u001b[39m\u001b[38;5;124m\"\u001b[39m, child\u001b[38;5;241m.\u001b[39mright_child\u001b[38;5;241m.\u001b[39mfeature_idx)\n",
      "Cell \u001b[0;32mIn[158], line 42\u001b[0m, in \u001b[0;36mtree_helper\u001b[0;34m(data_sub, cur_height, max_levels, cur_node)\u001b[0m\n\u001b[1;32m     40\u001b[0m child\u001b[38;5;241m.\u001b[39mright_child \u001b[38;5;241m=\u001b[39m tree_helper(right, new_height, max_levels, child)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated new left child with index: \u001b[39m\u001b[38;5;124m\"\u001b[39m, child\u001b[38;5;241m.\u001b[39mleft_child\u001b[38;5;241m.\u001b[39mfeature_idx)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated new right child with index: \u001b[39m\u001b[38;5;124m\"\u001b[39m, child\u001b[38;5;241m.\u001b[39mright_child\u001b[38;5;241m.\u001b[39mfeature_idx)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'feature_idx'"
     ]
    }
   ],
   "source": [
    "def create_decision_tree(data, max_levels):\n",
    "    my_feature, my_thresh = identify_best_split(data)\n",
    "    left, right = split_dataset(data, my_feature, my_thresh)\n",
    "    root_node = TreeNode\n",
    "\n",
    "    print(\"parent left length: \" , len(left))\n",
    "    print(\"parent right length: \" , len(right))\n",
    "    root_node.feature_idx = my_feature\n",
    "    root_node.is_leaf = False\n",
    "    print(\"ROOT split on \", my_feature)\n",
    "\n",
    "\n",
    "    root_node.left_child = tree_helper(left, 0, max_levels, root_node)\n",
    "    root_node.right_child = tree_helper(right, 0, max_levels, root_node)\n",
    "    return root_node\n",
    "\n",
    "def tree_helper(data_sub, cur_height, max_levels, cur_node) : \n",
    "    # Base case - all pure OR we've reached max height\n",
    "    # todo: what if 'all of the attributes are the same'\n",
    "    if (cur_height == max_levels or calc_entropy(data_sub) == 0) : \n",
    "        print(cur_node.feature_idx, \" reached base case\")\n",
    "        cur_node = create_leaf_node(data_sub)\n",
    "        return cur_node\n",
    "    child = TreeNode\n",
    "    child.is_leaf = False\n",
    "\n",
    "    my_feature, my_thresh = identify_best_split(data_sub)\n",
    "\n",
    "    left, right = split_dataset(data_sub, my_feature, my_thresh)\n",
    "    print(\"Child of parent feature  \", cur_node.feature_idx, \" split on new feature \", my_feature)\n",
    "\n",
    "\n",
    "    child.feature_idx = my_feature\n",
    "    child.thresh_val = my_thresh\n",
    "    print(\"child height \", cur_height, \" length left child: \", len(left))\n",
    "    print(\"child height \", cur_height, \" length right child: \", len(right))\n",
    "    new_height = cur_height + 1\n",
    "\n",
    "    child.left_child = tree_helper(left, new_height, max_levels, child)\n",
    "    child.right_child = tree_helper(right, new_height, max_levels, child)\n",
    "    print(\"created new left child with index: \", child.left_child.feature_idx)\n",
    "    print(\"created new right child with index: \", child.right_child.feature_idx)\n",
    "\n",
    "\n",
    "\n",
    "test_tree = create_decision_tree(silly_data, 50)\n",
    "# test_tree.is_leaf = False\n",
    "TreeNode.printTree(test_tree)\n",
    "\n",
    "print(test_tree.left_child.feature_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Given a test set, the function `calc_accuracy` returns the accuracy of the classifier. You'll use the `make_prediction` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(tree_root, data):\n",
    "#     your code goes here\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Keeping the `max_levels` parameter as 10, use 5-fold cross validation to measure the accuracy of the model. Print the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      8\u001b[0m test_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining set size:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_set))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest set size    :\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_set))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# the timer is just for fun! you will NOT be graded on runtime\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# edit the code here - this is just a sample to get you started\n",
    "import time\n",
    "\n",
    "d = get_data(\"messidor_features.txt\")\n",
    "\n",
    "# partition data into train_set and test_set\n",
    "train_set = None\n",
    "test_set = None\n",
    "\n",
    "print ('Training set size:', len(train_set))\n",
    "print ('Test set size    :', len(test_set))\n",
    "\n",
    "# the timer is just for fun! you will NOT be graded on runtime\n",
    "start = time.time()\n",
    "\n",
    "# create the decision tree\n",
    "tree = create_decision_tree(train_set, 10)\n",
    "\n",
    "end = time.time()\n",
    "print ('Time taken:', end - start)\n",
    "\n",
    "# calculate the accuracy of the tree\n",
    "accuracy = calc_accuracy(tree, test_set)\n",
    "print ('The accuracy on the test set is ', str(accuracy * 100.0))\n",
    "#t.printTree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
